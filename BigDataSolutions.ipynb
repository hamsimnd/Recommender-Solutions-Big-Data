{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae4f975-7fe5-4d73-9197-d46dbdead06b",
   "metadata": {},
   "source": [
    "# Huawei R&D Technical Interview Answer on Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89177f-745a-46c3-ae8b-b1dd54478b29",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of contents\n",
    "1. [Task – 1 Exploratory Data Analysis:](#EDA)\n",
    "2. [Task – 2 Recommender Design:](#RD)\n",
    "    1. [Step 1 : Recommender Model 1 Alternate Least Squares: Model Training and Evaluation](#step1)\n",
    "    2. [Step 2 : Recommender Model 2 KNN Algorithm: Model Training and Evaluation](#step2)    \n",
    "3. [Task – 3 Text Analysis:](#TA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd06e6d-7ea6-47a2-b6b1-bb1c97d6a9fc",
   "metadata": {},
   "source": [
    "## Task – 1 Exploratory Data Analysis:<a name=\"EDA\"></a>\n",
    "Writing queries with SQL-PYspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "79345dae-4e2b-493b-86f7-66bb17e3d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark\n",
    "## import findspark  \n",
    "## findspark.init()  \n",
    "## import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "de0ddadb-f08e-49f8-8bba-754d79776b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilazing Spark Session\n",
    "## First of all, a Spark session needs to be initialized. With the help of SparkSession, DataFrame can be created and registered as tables. Moreover, SQL tables are executed, tables can be cached For detailed explanations for each parameter of SparkSession, kindly visit https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.SparkSession.\n",
    "spark = SparkSession.builder.appName(\"HuaweiTechnicalInterview\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fce027ba-148b-4d3c-b46a-076bbd39e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Datasets (Implementing JSON File in PySpark)\n",
    "links = spark.read.option(\"multiline\",\"true\").json('C:/Users/kadir/Downloads/Huawei R&D Technical Interview Question on Big Data/links.json')\n",
    "ratings = spark.read.option(\"multiline\",\"true\").json('C:/Users/kadir/Downloads/Huawei R&D Technical Interview Question on Big Data/ratings.json')\n",
    "movies = spark.read.option(\"multiline\",\"true\").json('C:/Users/kadir/Downloads/Huawei R&D Technical Interview Question on Big Data/movies.json')\n",
    "tags = spark.read.option(\"multiline\",\"true\").json('C:/Users/kadir/Downloads/Huawei R&D Technical Interview Question on Big Data/tags.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d80f740-0b09-448d-9ac8-db381d68c91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- imdbId: string (nullable = true)\n",
      " |-- movieId: long (nullable = true)\n",
      " |-- tmdbId: string (nullable = true)\n",
      "\n",
      "+-------+-------+------+\n",
      "| imdbId|movieId|tmdbId|\n",
      "+-------+-------+------+\n",
      "|0114709|      1|   862|\n",
      "|0113497|      2|  8844|\n",
      "|0113228|      3| 15602|\n",
      "+-------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to verify that it was created correctly by viewing part of the dataset\n",
    "links.printSchema()\n",
    "links.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55e5a158-743a-4014-ac84-b952238ca7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: long (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- userId: long (nullable = true)\n",
      "\n",
      "+-------+---------+----------+------+\n",
      "|movieId|      tag| timestamp|userId|\n",
      "+-------+---------+----------+------+\n",
      "|      1|animation|1306926135|    40|\n",
      "|      1|  fantasy|1306926130|    40|\n",
      "|      1|    Pixar|1306926133|    40|\n",
      "+-------+---------+----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to verify that it was created correctly by viewing part of the dataset\n",
    "tags.printSchema()\n",
    "tags.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cb1f800-9b9e-4001-9bc3-a243f9e0a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- userId: long (nullable = true)\n",
      "\n",
      "+-------+------+---------+------+\n",
      "|movieId|rating|timestamp|userId|\n",
      "+-------+------+---------+------+\n",
      "|      6|   2.0|980730861|     1|\n",
      "|     22|   3.0|980731380|     1|\n",
      "|     32|   2.0|980731926|     1|\n",
      "+-------+------+---------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to verify that it was created correctly by viewing part of the dataset\n",
    "ratings.printSchema()\n",
    "ratings.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b5b3b3d-ee16-48af-bd93-848078989b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- genres: string (nullable = true)\n",
      " |-- movieId: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n",
      "+--------------------+-------+----------------+----+\n",
      "|              genres|movieId|           title|year|\n",
      "+--------------------+-------+----------------+----+\n",
      "|Adventure|Animati...|      1|       Toy Story|1995|\n",
      "|Adventure|Childre...|      2|         Jumanji|1995|\n",
      "|      Comedy|Romance|      3|Grumpier Old Men|1995|\n",
      "+--------------------+-------+----------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to verify that it was created correctly by viewing part of the dataset\n",
    "movies.printSchema()\n",
    "movies.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f12b81c1-cee5-47ae-a462-c68e097c53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0c25efd-4c17-4c58-9f06-ce79d0b4104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing task 1 queries \n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4d6fed5-5800-47f7-9239-4c511f60abe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+------+\n",
      "|userId|    title|              genres|rating|\n",
      "+------+---------+--------------------+------+\n",
      "|     7|Toy Story|Adventure|Animati...|   5.0|\n",
      "|    10|Toy Story|Adventure|Animati...|   4.0|\n",
      "|    13|Toy Story|Adventure|Animati...|   4.5|\n",
      "|    16|Toy Story|Adventure|Animati...|   5.0|\n",
      "|    21|Toy Story|Adventure|Animati...|   5.0|\n",
      "+------+---------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 1. Write a SQL query to create a dataframe with including userid, movieid, genre and rating\n",
    "### SQL Query %%sql SELECT userId,title,genres,rating FROM movies FULL OUTER JOIN ratings ON movies.movieId = ratings.movieId LIMIT 5; \n",
    "\n",
    "task1=ratings.join(movies, ratings.movieId == movies.movieId, 'outer').select(ratings.userId,movies.title,movies.genres,ratings.rating)\n",
    "task1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea228ca-ee1c-407b-8dab-47c6b6164972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------+\n",
      "|movieId|count(rating)|sum(rating)|\n",
      "+-------+-------------+-----------+\n",
      "|    318|          328|     1457.0|\n",
      "|    593|          337|     1427.5|\n",
      "|    296|          327|     1353.0|\n",
      "|    260|          306|     1284.0|\n",
      "|    356|          318|     1243.0|\n",
      "+-------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 2.Count ratings for each movie, and list top 5 movies with the highest value\n",
    "### SQL Query %%sql SELECT movieId,COUNT(rating),SUM(rating) FROM ratings GROUP BY movieId ORDER BY SUM(rating) DESC  LIMIT 5;   \n",
    "#Creating or replacing a local temporary view with this DataFrame.\n",
    "ratings.createOrReplaceTempView(\"ratings\")\n",
    "# Define my query\n",
    "query = \"SELECT movieId,COUNT(rating),SUM(rating) FROM ratings GROUP BY movieId ORDER BY SUM(rating) DESC  LIMIT 5\"\n",
    "newdf = spark.sql(query)\n",
    "#display the content of new dataframe\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d804f27b-2d1a-47f6-8c9a-db5767614102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|        genres|sum(rating)|\n",
      "+--------------+-----------+\n",
      "|         Drama|    25650.0|\n",
      "|        Comedy|    20529.5|\n",
      "|Comedy|Romance|    12930.5|\n",
      "| Drama|Romance|    11295.5|\n",
      "|  Comedy|Drama|    10709.5|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 3.Find and list top 5 most rated genres\n",
    "### SQL Query%%sql SELECT genres,SUM(rating) FROM movies LEFT OUTER JOIN ratings ON movies.movieId = ratings.movieId GROUP BY genres ORDER BY SUM(rating) DESC  LIMIT 5;  \n",
    "ratings.createOrReplaceTempView(\"ratings\")\n",
    "movies.createOrReplaceTempView(\"movies\")\n",
    "# Define my query\n",
    "task3 = \"SELECT genres,SUM(rating) FROM movies LEFT OUTER JOIN ratings ON movies.movieId = ratings.movieId GROUP BY genres ORDER BY SUM(rating) DESC  LIMIT 5;\"\n",
    "newtask3 = spark.sql(task3)\n",
    "#display the content of new dataframe\n",
    "newtask3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98b5d984-ab2f-4439-bd5a-37bd0670df2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|         tag|sum(rating)|\n",
      "+------------+-----------+\n",
      "|       drama|    14111.5|\n",
      "|twist ending|    12200.0|\n",
      "|      sci-fi|    11163.0|\n",
      "|  psychology|    10683.5|\n",
      "|       crime|     9973.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 4.- Find and list top 5 most rated tags\n",
    "### %%sql SELECT tag,SUM(rating) FROM tags LEFT OUTER JOIN ratings ON tags.movieId = ratings.movieId GROUP BY tag ORDER BY SUM(rating) DESC  LIMIT 5;  \n",
    "ratings.createOrReplaceTempView(\"ratings\")\n",
    "tags.createOrReplaceTempView(\"tags\")\n",
    "# Define my query\n",
    "task4 = \"SELECT tag,SUM(rating) FROM tags LEFT OUTER JOIN ratings ON tags.movieId = ratings.movieId GROUP BY tag ORDER BY SUM(rating) DESC  LIMIT 5; \"\n",
    "newtask4 = spark.sql(task4)\n",
    "#display the content of new dataframe\n",
    "newtask4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "285d84c8-b92a-41a9-95dc-9f2a96cbc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.By using timestamp from ratings table, provide top 5 most frequent users within a week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcdff1a2-ba6c-4e2f-b8a5-f3a9afdad227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|              genres|avg(rating)|\n",
      "+--------------------+-----------+\n",
      "|Action|Adventure|...|        5.0|\n",
      "|Crime|Documentary...|        5.0|\n",
      "|Adventure|Fantasy...|        5.0|\n",
      "|Animation|Documen...|        5.0|\n",
      "|Crime|Horror|Mystery|       4.75|\n",
      "|Comedy|Crime|Western|        4.5|\n",
      "|Animation|Comedy|...|        4.5|\n",
      "|Adventure|Comedy|...|        4.5|\n",
      "|Action|Animation|...|        4.5|\n",
      "|Action|Animation|...|        4.5|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 6.Calculate average ratings for each genre, and plot average ratings of top 10 genres with descending order\n",
    "### %%sql SELECT genres,AVG(rating) FROM movies LEFT OUTER JOIN ratings ON movies.movieId = ratings.movieId GROUP BY genres ORDER BY AVG(rating) DESC  LIMIT 10;\n",
    "ratings.createOrReplaceTempView(\"ratings\")\n",
    "movies.createOrReplaceTempView(\"movies\")\n",
    "# Define my query\n",
    "task6 = \"SELECT genres,AVG(rating) FROM movies LEFT OUTER JOIN ratings ON movies.movieId = ratings.movieId GROUP BY genres ORDER BY AVG(rating) DESC  LIMIT 10\"\n",
    "newtask6 = spark.sql(task6)\n",
    "#display the content of new dataframe\n",
    "newtask6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cddf72-6c66-4416-a944-d359c9ef443e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task – 2 Recommender Design: <a name=\"RD\"></a>\n",
    "The first paragraph text\n",
    "**<font color=red>*Recomendaiton System Question*</font>**\n",
    "- Provide an implicit feature by using any of the data from the given database\n",
    "- Train two individual recommender models, one by using rating (from ratings table) and the other one by using your designed implicit feedback\n",
    "- Present comparison between two models, by using essential metrics\n",
    "\n",
    "**<font color=red>*Recomendaiton System Explanaiton-1*</font>**\n",
    "Recommender systems are machine learning-based systems that scan through all possible options and provides a prediction or recommendation. However, building a recommendation system has the below complications:\n",
    "\n",
    "- Users’ data is interchangeable.\n",
    "- The data volume is large and includes a significant list of movies, shows, customers’ profiles and interests, ratings, and other data points.\n",
    "- New registered customers use to have very limited information.\n",
    "- Real-time prediction for users.\n",
    "- Old users can have an overabundance of information.\n",
    "- It should not show items that are very different or too similar.\n",
    "- Users can change the rating of items on change of his/her mind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2aa2ac-05c9-477d-a53e-abf4cc8eb64e",
   "metadata": {},
   "source": [
    "### Step 1 : Recommender Model 1 Alternate Least Squares: Model Training and Evaluation <a name=\"step1\"></a>\n",
    "**<font color=Blue>*ALS*</font>** is one of the low rank matrix approximation algorithms for collaborative filtering. ALS decomposes user-item matrix into two low rank matrixes: user matrix and item matrix. In collaborative filtering, users and products are described by a small set of latent factors that can be used to predict missing entries. And ALS algorithm learns these latent factors by matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef718244-283d-4e26-a535-6deb29a0be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing Essential Libraries for machine learning processes\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7a443-c8af-464c-a3c4-e105fc6ff1b6",
   "metadata": {},
   "source": [
    "### Train Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a605590-8852-4016-a646-adb1b2a8a6b6",
   "metadata": {},
   "source": [
    "Use the randomSplit function to divide the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "920939d4-4cb3-47b4-9ea9-b2d73884f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = ratings.randomSplit([0.7,0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af6b1179-4b90-4d71-8831-503f2bea3ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+\n",
      "|summary|          movieId|            rating|           timestamp|            userId|\n",
      "+-------+-----------------+------------------+--------------------+------------------+\n",
      "|  count|            70032|             70032|               70032|             70032|\n",
      "|   mean|8544.972626799177|3.4908184829792095|1.0909841194480524E9| 342.0761223440713|\n",
      "| stddev|19621.04239062862|1.0671878312293106|1.6328108103097975E8|193.84742349370407|\n",
      "|    min|                1|               0.5|           828504918|                 1|\n",
      "|    max|           129651|               5.0|          1427754939|               706|\n",
      "+-------+-----------------+------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e45c0d2-de0a-4373-a477-4102b2e9792d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+------------------+\n",
      "|summary|           movieId|            rating|          timestamp|            userId|\n",
      "+-------+------------------+------------------+-------------------+------------------+\n",
      "|  count|             29991|             29991|              29991|             29991|\n",
      "|   mean| 8772.262445400287|3.4926311226701343|1.092772549526858E9| 341.0243739788603|\n",
      "| stddev|20001.310616469647|1.0697166979508876|1.635322738786036E8|193.85643213001717|\n",
      "|    min|                 1|               0.5|          828504918|                 1|\n",
      "|    max|            126407|               5.0|         1427752633|               706|\n",
      "+-------+------------------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75371f52-ab81-4e79-bbcd-28784fbde434",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "### Build the recommendation model using ALS on the training data\\\\ Now let's build our ALS model and fit this model with training features created by randomsplit() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8cc384a-786a-41ce-ac6b-d8dfb4dce16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ae62d21-661a-4016-964f-2d4d1a720ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7648c694-2010-4e16-afce-31bcd9c26a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+------+----------+\n",
      "|movieId|rating| timestamp|userId|prediction|\n",
      "+-------+------+----------+------+----------+\n",
      "|      1|   3.0|1273228128|   271| 3.3592305|\n",
      "|      1|   3.5|1130850782|   663| 3.7884517|\n",
      "|      1|   3.5|1306926127|    40| 4.1003222|\n",
      "|      1|   3.0| 864828661|   340| 3.3957133|\n",
      "|      1|   4.0| 834767668|   647| 5.2124333|\n",
      "|      1|   3.0| 944340594|    92|  3.358765|\n",
      "|      1|   4.0| 832447335|   489|  4.134621|\n",
      "|      1|   3.0| 894902672|    72| 3.7634354|\n",
      "|      1|   3.5|1175935476|   310| 3.6029649|\n",
      "|      1|   1.0|1328400944|    84| 2.9836214|\n",
      "|      1|   3.0| 867338979|   171| 2.7192297|\n",
      "|      1|   3.0| 865287270|   395| 3.6687374|\n",
      "|      1|   3.0| 975602509|   394| 3.2908976|\n",
      "|      1|   3.0| 866736804|   156| 2.9144173|\n",
      "|      1|   3.0| 850988642|   653| 3.0380714|\n",
      "|      1|   3.5|1296441497|   550|  4.038661|\n",
      "|      1|   3.5|1232916522|   256| 3.4899235|\n",
      "|      1|   3.0|1046140820|   105|  4.069527|\n",
      "|      1|   3.0|1008864246|   106| 3.3208418|\n",
      "|      1|   3.5|1085087936|   123| 3.4147894|\n",
      "+-------+------+----------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Generating Predictions & Model Evaluation\n",
    "predictions = model.transform(test)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b8814eb-f52c-428d-a113-8d60b0b98d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = nan\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "210a65bb-f411-459f-90e2-929b2e476a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rating in the dataset is: 3.491361986743049\n"
     ]
    }
   ],
   "source": [
    "avgRatings = ratings.select('rating').groupBy().avg().first()[0]\n",
    "print ('The average rating in the dataset is: {}'.format(avgRatings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1bef15-6f7d-44ee-8ed4-39d124dbb83d",
   "metadata": {
    "tags": []
   },
   "source": [
    "So now that we have the model, how would you actually supply a recommendation to a user?\n",
    "The approach here will be simple We will be taking a single userid example11 as features and pass it to trained ALS Model. The same way we did with the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71e86f43-0fc0-4ba0-9a9e-b69c9100d69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|movieId|userId|\n",
      "+-------+------+\n",
      "|     19|    11|\n",
      "|    235|    11|\n",
      "|    553|    11|\n",
      "|   3793|    11|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_user = test.filter(test['userId']==11).select(['movieId','userId'])\n",
    "# User had 10 ratings in the test data set \n",
    "# Realistically this should be some sort of hold out set!\n",
    "single_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9102f8d-9f64-452a-bcf1-3fc4dfb4fcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+\n",
      "|movieId|userId|prediction|\n",
      "+-------+------+----------+\n",
      "|    235|    11|  6.762187|\n",
      "|     19|    11| 3.8154223|\n",
      "|   3793|    11| 3.7515554|\n",
      "|    553|    11|  2.880481|\n",
      "+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recomendations = model.transform(single_user)\n",
    "recomendations.orderBy('prediction',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4e419128-c822-49c7-acf9-6ecf1771a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+----+-------+------+----------+\n",
      "|              genres|cmovieId|               title|year|movieId|userId|prediction|\n",
      "+--------------------+--------+--------------------+----+-------+------+----------+\n",
      "|Action|Adventure|...|    3793|               X-Men|2000|   3793|    11| 3.7515554|\n",
      "|        Comedy|Drama|     235|             Ed Wood|1994|    235|    11|  6.762187|\n",
      "|              Comedy|      19|Ace Ventura: When...|1995|     19|    11| 3.8154223|\n",
      "|Action|Drama|Western|     553|           Tombstone|1993|    553|    11|  2.880481|\n",
      "+--------------------+--------+--------------------+----+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recomendations.createOrReplaceTempView(\"recomendations\")\n",
    "movies.createOrReplaceTempView(\"movies\")\n",
    "# Define my query\n",
    "singleuser1 = \"SELECT * FROM movies LEFT OUTER JOIN recomendations ON movies.cmovieId = recomendations.movieId where userId=11;\"\n",
    "newsingleuser1 = spark.sql(singleuser1)\n",
    "#display the content of new dataframe\n",
    "newsingleuser1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffee819-6841-492c-b538-9127be427782",
   "metadata": {},
   "source": [
    "### Step 2 : Recommender Model 2 KNN Algorithm: Model Training and Evaluation <a name=\"step1\"></a>\n",
    "Collaborative filtering based systems use the actions of users to recommend other items. In general, they can either be user based or item based. User based collaborating filtering uses the patterns of users similar to me to recommend a product (users like me also looked at these other items). Item based collaborative filtering uses the patterns of users who browsed the same item as me to recommend me a product (users who looked at my item also looked at these other items). Item-based approach is usually prefered than user-based approach. User-based approach is often harder to scale because of the dynamic nature of users, whereas items usually don't change much, so item-based approach often can be computed offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a0409c66-fcfc-47bf-8c6e-4b955138fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=movies.withColumnRenamed(\"movieId\",\"cmovieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "67a7c851-5508-4867-8842-6e6067dfcf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------------------+----+-------+------+----------+------+\n",
      "|genres|cmovieId|               title|year|movieId|rating| timestamp|userId|\n",
      "+------+--------+--------------------+----+-------+------+----------+------+\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   3.0| 831728688|   685|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   2.0|1051770941|   677|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   4.0|1247814964|   668|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   3.0|1127249360|   663|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   3.0| 834610377|   662|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   2.0| 836221196|   656|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   3.5|1137595314|   639|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   3.0| 839937911|   620|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   2.0|1134735487|   616|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   4.0| 833033911|   615|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   3.0|1205271122|   581|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   2.0| 905465240|   570|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   4.0|1104638913|   566|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   1.0|1105032504|   555|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   1.0| 841136608|   553|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   3.0| 843474088|   549|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   2.0|1198469735|   544|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   1.5|1181316946|   529|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   1.0| 970542224|   526|\n",
      "|Comedy|      19|Ace Ventura: When...|1995|     19|   3.0| 861878010|   524|\n",
      "+------+--------+--------------------+----+-------+------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.createOrReplaceTempView(\"ratings\")\n",
    "movies.createOrReplaceTempView(\"movies\")\n",
    "# Define my query\n",
    "forKNN = \"SELECT * FROM movies LEFT OUTER JOIN ratings ON movies.cmovieId = ratings.movieId;\"\n",
    "newforKNN = spark.sql(forKNN)\n",
    "#display the content of new dataframe\n",
    "newforKNN.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "70127757-4cae-48a6-97b3-5573792efbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_movie_rating = newforKNN.na.drop( subset = ['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7891534b-e889-4c74-b839-1fb64858f3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|               title|totalRatingCount|\n",
      "+--------------------+----------------+\n",
      "|Silence of the La...|             337|\n",
      "|Shawshank Redempt...|             328|\n",
      "|        Pulp Fiction|             327|\n",
      "|       Jurassic Park|             324|\n",
      "|        Forrest Gump|             318|\n",
      "|Star Wars: Episod...|             306|\n",
      "|          Braveheart|             292|\n",
      "|Terminator 2: Jud...|             278|\n",
      "|         Matrix, The|             265|\n",
      "|       Fugitive, The|             250|\n",
      "|Star Wars: Episod...|             246|\n",
      "|Star Wars: Episod...|             246|\n",
      "|           Apollo 13|             244|\n",
      "|Independence Day ...|             242|\n",
      "|    Schindler's List|             241|\n",
      "| Usual Suspects, The|             239|\n",
      "|              Batman|             235|\n",
      "|           Toy Story|             232|\n",
      "|  Dances with Wolves|             227|\n",
      "|     American Beauty|             221|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define my query\n",
    "combine_movie_rating.createOrReplaceTempView(\"combine_movie_rating\")\n",
    "movie_ratingCount = \"SELECT title,count(rating) as totalRatingCount FROM combine_movie_rating GROUP BY title ORDER BY count(rating) DESC  \"\n",
    "newmovie_ratingCount = spark.sql(movie_ratingCount)\n",
    "#display the content of new dataframe\n",
    "newmovie_ratingCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9d8bc5a4-c70d-48b6-9cdb-6c3f9d40999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newmovie_ratingCount=newmovie_ratingCount.withColumnRenamed(\"title\",\"ctitle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "76770f3c-55a8-45b0-b535-e8de6fcb66fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+------+----------------+--------------------+\n",
      "|userId|movieId|               title|rating|totalRatingCount|              genres|\n",
      "+------+-------+--------------------+------+----------------+--------------------+\n",
      "|     1|    593|Silence of the La...|   5.0|             337|Crime|Horror|Thri...|\n",
      "|     7|    593|Silence of the La...|   4.0|             337|Crime|Horror|Thri...|\n",
      "|     8|    593|Silence of the La...|   5.0|             337|Crime|Horror|Thri...|\n",
      "|     9|    593|Silence of the La...|   4.0|             337|Crime|Horror|Thri...|\n",
      "|    10|    593|Silence of the La...|   3.5|             337|Crime|Horror|Thri...|\n",
      "|    12|    593|Silence of the La...|   5.0|             337|Crime|Horror|Thri...|\n",
      "|    14|    593|Silence of the La...|   4.0|             337|Crime|Horror|Thri...|\n",
      "|    18|    593|Silence of the La...|   5.0|             337|Crime|Horror|Thri...|\n",
      "|    26|    593|Silence of the La...|   5.0|             337|Crime|Horror|Thri...|\n",
      "|    30|    593|Silence of the La...|   4.0|             337|Crime|Horror|Thri...|\n",
      "|    31|    593|Silence of the La...|   4.0|             337|Crime|Horror|Thri...|\n",
      "|    34|    593|Silence of the La...|   4.0|             337|Crime|Horror|Thri...|\n",
      "|    36|    593|Silence of the La...|   4.5|             337|Crime|Horror|Thri...|\n",
      "|    37|    593|Silence of the La...|   4.5|             337|Crime|Horror|Thri...|\n",
      "|    42|    593|Silence of the La...|   2.0|             337|Crime|Horror|Thri...|\n",
      "|    43|    593|Silence of the La...|   4.0|             337|Crime|Horror|Thri...|\n",
      "|    44|    593|Silence of the La...|   4.5|             337|Crime|Horror|Thri...|\n",
      "|    45|    593|Silence of the La...|   3.0|             337|Crime|Horror|Thri...|\n",
      "|    47|    593|Silence of the La...|   3.0|             337|Crime|Horror|Thri...|\n",
      "|    48|    593|Silence of the La...|   4.0|             337|Crime|Horror|Thri...|\n",
      "+------+-------+--------------------+------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combine_movie_rating.createOrReplaceTempView(\"combine_movie_rating\")\n",
    "newmovie_ratingCount.createOrReplaceTempView(\"newmovie_ratingCount\")\n",
    "# Define my query\n",
    "rating_with_totalRatingCount = \"SELECT userId, movieId,title,rating,totalRatingCount,genres FROM newmovie_ratingCount LEFT OUTER JOIN combine_movie_rating ON newmovie_ratingCount.ctitle = combine_movie_rating.title;\"\n",
    "newrating_with_totalRatingCount = spark.sql(rating_with_totalRatingCount)\n",
    "#display the content of new dataframe\n",
    "newrating_with_totalRatingCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "952258f5-5645-450a-98f5-037355a57874",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = newrating_with_totalRatingCount.groupBy(\"title\",\"userId\") \\\n",
    "      .sum(\"rating\") \\\n",
    "      .groupBy(\"title\") \\\n",
    "      .pivot(\"userId\") \\\n",
    ".count()\n",
    "\n",
    "\n",
    "movie_features_df=newrating_with_totalRatingCount.pivot_table(index='title',columns='userId',values='rating').fillna(0)\n",
    "movie_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "547dd452-2be5-41d4-ad47-d43c9c2a7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "newrating_with_totalRatingCount = newrating_with_totalRatingCount.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "423a8397-552f-4fbf-8777-69a7ca56e250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>userId</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>10.0</th>\n",
       "      <th>...</th>\n",
       "      <th>697.0</th>\n",
       "      <th>698.0</th>\n",
       "      <th>699.0</th>\n",
       "      <th>700.0</th>\n",
       "      <th>701.0</th>\n",
       "      <th>702.0</th>\n",
       "      <th>703.0</th>\n",
       "      <th>704.0</th>\n",
       "      <th>705.0</th>\n",
       "      <th>706.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>\"Great Performances\" Cats</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Til There Was You</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'burbs, The</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'night Mother</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(500) Days of Summer</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 706 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "userId                     1.0    2.0    3.0    4.0    5.0    6.0    7.0    \\\n",
       "title                                                                        \n",
       "\"Great Performances\" Cats    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "'Til There Was You           0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "'burbs, The                  0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "'night Mother                0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "(500) Days of Summer         0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "userId                     8.0    9.0    10.0   ...  697.0  698.0  699.0  \\\n",
       "title                                           ...                        \n",
       "\"Great Performances\" Cats    0.0    0.0    0.0  ...    0.0    0.0    0.0   \n",
       "'Til There Was You           0.0    0.0    0.0  ...    0.0    0.0    0.0   \n",
       "'burbs, The                  0.0    0.0    0.0  ...    0.0    0.0    0.0   \n",
       "'night Mother                0.0    0.0    0.0  ...    0.0    0.0    0.0   \n",
       "(500) Days of Summer         0.0    0.0    0.0  ...    0.0    0.0    0.0   \n",
       "\n",
       "userId                     700.0  701.0  702.0  703.0  704.0  705.0  706.0  \n",
       "title                                                                       \n",
       "\"Great Performances\" Cats    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "'Til There Was You           0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "'burbs, The                  0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "'night Mother                0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "(500) Days of Summer         0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 706 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_features_df=newrating_with_totalRatingCount.pivot_table(index='title',columns='userId',values='rating').fillna(0)\n",
    "movie_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3a6be8b3-3acf-4fee-9921-602202575d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='brute', metric='cosine')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "movie_features_df_matrix = csr_matrix(movie_features_df.values)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')\n",
    "model_knn.fit(movie_features_df_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ea45576b-dcad-4c21-8179-59ae4fca3103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1928\n"
     ]
    }
   ],
   "source": [
    "query_index = np.random.choice(movie_features_df.shape[0])\n",
    "print(query_index)\n",
    "query_index =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "747a1408-2ac6-421d-9c00-94ba26710768",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = model_knn.kneighbors(movie_features_df.iloc[query_index,:].values.reshape(1, -1), n_neighbors = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c827c3bf-318b-47a7-8d66-e081c2d96b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for 'Til There Was You:\n",
      "\n",
      "1: Plenty, with distance of 0.05131670194948623:\n",
      "2: Stealing Home, with distance of 0.05131670194948623:\n",
      "3: Children of the Revolution, with distance of 0.05131670194948623:\n",
      "4: Governess, The, with distance of 0.05131670194948623:\n",
      "5: Love Serenade, with distance of 0.05131670194948623:\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(distances.flatten())):\n",
    "    if i == 0:\n",
    "        print('Recommendations for {0}:\\n'.format(movie_features_df.index[query_index]))\n",
    "    else:\n",
    "        print('{0}: {1}, with distance of {2}:'.format(i, movie_features_df.index[indices.flatten()[i]], distances.flatten()[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d9e43b-3e7f-4ae0-926a-cf7ffb2aecf6",
   "metadata": {},
   "source": [
    "-  **<font color=Blue>To sum up,</font>** In this study, a recommendation system was prepared with two different approaches. The first is user-based and offers suggestions for users, while the second study is film-based and offers suggestions for movies that are close to that movie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5c55e-2509-4c5e-9a5c-50bc0c1db85e",
   "metadata": {},
   "source": [
    "## Task – 3 Text Analysis: <a name=\"TA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed6b4a-8f03-49ee-87c4-2af063b7e81b",
   "metadata": {},
   "source": [
    "- Create a dataframe with following schema:\n",
    "root\n",
    "|-- content: string (nullable = true)\n",
    "|-- label: string (nullable = true)\n",
    "|-- sentiment: string (nullable = true)\n",
    "- Design a tokenizer for content column and remove stop words, and give descriptive information\n",
    "about obtained content column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035ec43-e315-4648-9061-4e8f000685ea",
   "metadata": {},
   "source": [
    "#### Step 1 - Import nltk and download stopwords, and then import stopwords from NLTK\n",
    "- nltk is the most popular Python package for Natural Language processing, it provides algorithms for importing, cleaning, pre-processing text data in human language and then apply computational linguistics algorithms like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "510b5d85-e547-4433-b983-b64526dafdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kadir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import glob\n",
    "import shutil\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "eng_stopwords = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "7b9a3f07-92e1-48a8-a688-06f44261853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, sys \n",
    "from shutil import make_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788be050-d502-4199-b6d4-5f04b95d9627",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "The data used in this model study data is made up of publicly shared locations. There is one analyst in the list, there is a list of people who are looking for training, that is, a comparison list, and there is a test list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "702d3c3b-d313-4460-8692-362edeaf47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953709ae-e044-427c-a1a8-bf559d0b17c3",
   "metadata": {},
   "source": [
    "To prepare a dataset for binary classification, you will need two folders on disk, corresponding to class_a and class_b. These will be the positive and negative movie reviews, which can be found in aclImdb/train/pos and aclImdb/train/neg. As the IMDB dataset contains additional folders, you will remove them before using this utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "4884ce77-3b92-4a39-a288-39453a8d15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "4afc2a2e-5c75-4281-bde0-85ada75b8c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "7dcfa70f-7221-4c3c-b255-b4e15887571c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db206edb-e662-4163-ba18-c14209db075c",
   "metadata": {},
   "source": [
    "-  **<font color=purple>The aclImdb/train/pos and aclImdb/train/neg directories contain many text files, each of which is a single movie review. Let's take a look at one of them</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "a2285ba2-f06b-432d-ac5a-f10611958654",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
    "with open(sample_file) as f:\n",
    "    txt=f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "f5998317-707c-44be-8b4e-d9ddb5243c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\""
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "b0a2a5b8-5c16-45f0-9dae-9c51b112edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "d92d2e69-9e09-4492-a131-bd3b9801b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create expected schema\n",
    "\n",
    "expectedSchema = StructType([\n",
    "  StructField(\"content\", StringType(), True),\n",
    "  StructField(\"label\", StringType(), True),\n",
    "  StructField(\"sentiment\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "169675eb-f2f5-43ef-9f3d-8b5cfcc108ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(txt,\"0\",\"neg\")  ]\n",
    "\n",
    "df = spark.createDataFrame(data=data2,schema=expectedSchema)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012216a3-4118-4cd6-8038-c41313a32113",
   "metadata": {},
   "source": [
    "-  **<font color=purple>The first dataframe creation process in task 3 was carried out through an example. In this context, the expected schema was created and the schema was shown according to the specified data contents.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602e895-5982-4515-ae2e-884ab20109d6",
   "metadata": {},
   "source": [
    "Next, you will use the text_dataset_from_directory utility to create a labeled tf.data.Dataset. tf.data is a powerful collection of tools for working with data.\n",
    "\n",
    "When running a machine learning experiment, it is a best practice to divide your dataset into three splits: train, validation, and test.\n",
    "\n",
    "The IMDB dataset has already been divided into train and test, but it lacks a validation set. Let's create a validation set using an 80:20 split of the training data by using the validation_split argument below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "cb0b1160-5a23-401b-98ff-fbdda39cbdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d005792-9902-4222-88e0-cb97d37426ea",
   "metadata": {},
   "source": [
    "As you can see above, there are 25,000 examples in the training folder, of which you will use 80% (or 20,000) for training. As you will see in a moment, you can train a model by passing a dataset directly to model.fit. If you're new to tf.data, you can also iterate over the dataset and print out a few examples as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "a7221656-0919-46cd-a20a-6299b6721801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content b'Every scene was put together perfectly.This movie had a wonderful cast and crew. I mean, how can you have a bad movie with Robert Downey Jr. in it,none have and ever will exist. He has the ability to brighten up any movie with his amazing talent.This movie was perfect! I saw this movie sitting all alone on a movie shelf in \"Blockbuster\" and like it was calling out to me,I couldn\\'t resist picking it up and bringing it home with me. You can call me a sappy romantic, but this movie just touched my heart, not to mention made me laugh with pleasure at the same time. Even though it made me cry,I admit, at the end, the whole movie just brightened up my outlook on life thereafter.I suggested to my horror, action, and pure humor movie buff of a brother,who absolutely adored this movie. This is a movie with a good sense of feeling.It could make you laugh out loud, touch your heart, make you fall in love,and enjoy your life.Every time you purposefully walk past this movie, just be aware that you are consciously making the choice to live and feel this inspiring movie.Who knows? What if it could really happen to you?, and keep your mind open to the mystical wonders of life.'\n",
      "Label 0\n",
      "\n",
      "\n",
      "Content b\"'They All Laughed' is a superb Peter Bogdanovich that is finally getting the recognition it deserves, and why? their are many reasons the fact that it's set in new york which truly sets the tone, the fantastic soundtrack, the appealing star turns from Ben Gazzara, and the late John Ritter who is superb. and of course no classic is complete without Audrey Hepburn. the film is a light and breezy romantic comedy that is very much in the vein of screwball comedy from the thirties, film is essentially about the Odyssey detective agency which is run by Gazzara who with his fellow detectives pot smoking and roller skating eccentric Blaine Novak(the films co-producer) and John Ritter, basically the Gazzara falls for a rich tycoon magnate's wife(Hepburn) and Ritter falls for beautiful Dorothy Stratten who sadly murdered infamously after production, 'They All Laughed is essential viewing for Bogdanovich fans.\"\n",
      "Label 0\n",
      "\n",
      "\n",
      "Content b\"I agree with the above comment, I love the realism in this, and in many movies (not just movies on eating disorders) the producers seem to forget that. They take an every day problem and create a hugely dramatic scene and then come the end of the movie everything is perfect again, which I dislike because its not reality. Not meaning to say things can't get better, and not meaning to say things don't in this movie, but it doesn't spend most of the movie creating all these problems, and come the end of the movie everything is perfect again. When people have eating disorders people don't just admit it and want to get better, and then life is peachy, it takes time, and I like how in this movie we grow with the characters, we go through the difficulties with them, getting better and worse, because it is a very important part of the movie. It gets into the minds of people with eating disorders, and shows the complications and pain, in a very realistic way, and I loved that. I also love how it shows The secrecy and betrayal people feel when suffering from eating disorders, it is scary to see how people react when they find out, especially if they approve of it. I thought this movie was very touching and beautiful and well told, and defiantly one of my favourites.\"\n",
      "Label 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text_bathc, label_batch, in raw_train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(\"Content\", text_batch.numpy()[i])\n",
    "    print(\"Label\", label_batch.numpy()[i])\n",
    "    print('\\n')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a830af-cd71-45f2-a8ef-8e9f32e35709",
   "metadata": {},
   "source": [
    "Sentiment analysis aims to estimate the sentiment polarity of a body of text based solely on its content. The sentiment polarity of text can be defined as a value that says whether the expressed opinion is positive (polarity=1), negative (polarity=0),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "b753a4a8-c312-43d4-b1c1-00326baf6a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to neg\n",
      "Label 1 corresponds to pos\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5415f64-a66f-4b6c-b479-8986a9cd9193",
   "metadata": {},
   "source": [
    "With the method shown above, we can categorize the data as content label and sentiment in the main data.\n",
    "\n",
    "If we were building the model, in the next steps, we would create a validation and test dataset.\n",
    "and we would use the remaining 5,000 reviews from the training set for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "10110c5b-066c-43d6-b577-25f3f8c97f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "6b782ffc-cf05-4f9f-9558-3f94fe19871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', \n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "23e6290c-26c3-4906-9e56-280f6ddb40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data preprocessing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358bdcf1-3c4a-4cc5-81fb-e30de127db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having looked at our data above, we see that the raw text contains HTML break\n",
    "# tags of the form '<br />'. These tags will not be removed by the default\n",
    "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
    "# create a custom standardization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "f4731de5-0deb-41ee-9ef3-64062ba3c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  # lowercase the input string\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  # remove html tags\n",
    "  strip_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  # remove punctuations\n",
    "  return tf.strings.regex_replace(strip_html, \n",
    "                                  '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "1d9177d7-26f4-42ab-9446-b6831032b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorization layer for text data\n",
    "max_features = 10000\n",
    "seq_length = 224\n",
    "\n",
    "vectorization_layer = TextVectorization(\n",
    "    standardize = custom_standardization,\n",
    "    max_tokens = max_features,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db230ac9-fc92-4f17-9449-c91cf6c4765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our custom standardization, we can instantiate our text\n",
    "# vectorization layer. We are using this layer to normalize, split, and map\n",
    "# strings to integers, so we set our 'output_mode' to 'int'.\n",
    "# Note that we're using the default split function,\n",
    "# and the custom standardization defined above.\n",
    "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
    "# model won't support ragged sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "724019be-025f-4e4a-bf7f-f23ee7bd8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorization_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf3ca5-bf2d-4128-9965-2bb9ca2d5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the vocab layer has been created, call `adapt` on a text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
    "# datasets this means you're not keeping spare copies of the dataset in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "733b93f4-472b-4ec7-8a57-78342f7ca822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorization layer for text data\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "max_features = 10000\n",
    "seq_length = 224\n",
    "\n",
    "vectorization_layer = TextVectorization(\n",
    "    standardize = custom_standardization,\n",
    "    max_tokens = max_features,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1554a3-afa6-4f20-80e1-62657bdad007",
   "metadata": {},
   "source": [
    "Call adapt to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "fa68af9f-c658-4cd2-96c5-8c0f0b92ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a text-only dataset (without labels) and call adapt\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "60648a3f-3537-47ba-aaf6-361593887dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorization_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "39361624-3aac-488e-92f7-b93345f60ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'Great movie - especially the music - Etta James - \"At Last\". This speaks volumes when you have finally found that special someone.'\n",
      "Label: neg\n",
      "Vectorized Review: (<tf.Tensor: shape=(1, 224), dtype=int64, numpy=\n",
      "array([[  85,   17,  260,    2,  222,    1,  571,   31,  229,   11, 2421,\n",
      "           1,   51,   22,   25,  404,  251,   12,  308,  282,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "# get a batch of 32 reviews and labels from the dataset\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(f'Review: {first_review}')\n",
    "print(f'Label: {raw_train_ds.class_names[first_label]}')\n",
    "print(f'Vectorized Review: {vectorize_text(first_review, first_label)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "568cc2e6-06ec-4b72-af33-16f21e8b3d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021 --->  cop\n",
      "101 --->  after\n",
      "Vocabulary Size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"1021 ---> \", vectorization_layer.get_vocabulary()[1021])\n",
    "print(\"101 ---> \", vectorization_layer.get_vocabulary()[101])\n",
    "print('Vocabulary Size: {}'.format(len(vectorization_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4cd2c-8639-4962-82b5-185d65c492d5",
   "metadata": {},
   "source": [
    "- Apply the TextVectorization layer to the train, validation, and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "046c8d5d-430f-4543-97f2-80c21201e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "32f473e9-9dd1-4445-a4b2-6d4a5ab21e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cefe69b-3cc8-43f6-8aa8-d0e1b61a2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "-  **<font color=purple>The first dataframe creation process in task 3 was carried out through an example. In this context, the expected schema was created and the schema was shown according to the specified data contents.</font>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
